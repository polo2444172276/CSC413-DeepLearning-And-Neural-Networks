{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing RNNs and LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd\n",
    "import autograd.misc.optimizers as optim\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "You may find the following resources helpful for understanding how RNNs and LSTMs work:\n",
    "\n",
    "* [The Unreasonable Effectiveness of RNNs (Andrej Karpathy)](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "* [Recurrent Neural Networks Tutorial (Wild ML)](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)\n",
    "* [Understanding LSTM Networks (Chris Olah)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-Level Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will reproduce Andrej Karpathy's character level language model. Given a specific character, we use a RNN/LSTM to learn the distribution of the next character. We apply the character level language model on a dataset of Shakespearian text, located at: https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "The tutorial is broken into the following sections:\n",
    "- Part 1: Implemention of RNNs from scratch and application on the dataset.\n",
    "- Part 2: Implemention LSTMs from scratch and application on the dataset.\n",
    "- Part 3: Implementation of RNN in PyTorch, and example of batch training using sequential data.\n",
    "- Part 4: Strategies for batch training irregular length time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "The vocabulary contains ['m', 'T', 'Y', 'w', 'E', 'Q', 'f', '!', 'I', '$', 's', ';', 'g', 'K', 'y', 'B', ':', 'N', 'q', 'x', 'l', 'L', 'i', 'X', \"'\", 'u', 'd', 't', 'k', 'v', 'M', 'p', 'J', 'e', 'c', '.', 'o', 'W', 'n', 'b', ',', 'O', 'j', 'Z', '-', 'P', 'S', 'G', 'A', 'a', 'H', 'D', 'C', ' ', 'h', '3', 'U', 'F', 'V', '\\n', '&', 'r', 'R', '?', 'z']\n",
      "------------------------------\n",
      "TOTAL NUM CHARACTERS = 1115394\n",
      "NUM UNIQUE CHARACTERS = 65\n",
      "char_to_index {'m': 0, 'T': 1, 'Y': 2, 'w': 3, 'E': 4, 'Q': 5, 'f': 6, '!': 7, 'I': 8, '$': 9, 's': 10, ';': 11, 'g': 12, 'K': 13, 'y': 14, 'B': 15, ':': 16, 'N': 17, 'q': 18, 'x': 19, 'l': 20, 'L': 21, 'i': 22, 'X': 23, \"'\": 24, 'u': 25, 'd': 26, 't': 27, 'k': 28, 'v': 29, 'M': 30, 'p': 31, 'J': 32, 'e': 33, 'c': 34, '.': 35, 'o': 36, 'W': 37, 'n': 38, 'b': 39, ',': 40, 'O': 41, 'j': 42, 'Z': 43, '-': 44, 'P': 45, 'S': 46, 'G': 47, 'A': 48, 'a': 49, 'H': 50, 'D': 51, 'C': 52, ' ': 53, 'h': 54, '3': 55, 'U': 56, 'F': 57, 'V': 58, '\\n': 59, '&': 60, 'r': 61, 'R': 62, '?': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "# Load the Shakespeare text. See above for download link.\n",
    "with open('./data/shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"------------------------------\")\n",
    "# Print a sample of the text\n",
    "print(text[:200])\n",
    "data_length = len(text)\n",
    "vocab = list(set(text))\n",
    "vocab_size = len(vocab)   # + 1      # The extra + 1 is for the end-of-string token\n",
    "\n",
    "char_to_index = { char:index for (index,char) in enumerate(vocab) }\n",
    "index_to_char = { index:char for (index,char) in enumerate(vocab) }\n",
    "\n",
    "print(\"The vocabulary contains {}\".format(vocab))\n",
    "print(\"------------------------------\")\n",
    "print(\"TOTAL NUM CHARACTERS = {}\".format(data_length))\n",
    "print(\"NUM UNIQUE CHARACTERS = {}\".format(vocab_size))\n",
    "print('char_to_index {}'.format(char_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: RNNs\n",
    "\n",
    "![Recurrent Neural Network Diagram](http://www.wildml.com/wp-content/uploads/2015/09/rnn.jpg)\n",
    "(Image from the [Wild ML RNN Tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/))\n",
    "\n",
    "The update of an RNN is expressed by the following formulas:\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(U x_t + W h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_t = \\text{softmax}(V h_t + b_y)\n",
    "$$\n",
    "\n",
    "Here, each $x_t$ is a _character_---in this example, there are 65 unique characters. Since in each step the model takes as input a character and outputs a prediction for the next character in the sequence, both $x_t$ and $o_t$ are 65-dimensional vectors, i.e., $x_t, o_t \\in \\mathbb{R}^{65}$. We can choose any dimension for the hidden state $h_t$; in this case, we will use $h_t \\in \\mathbb{R}^{100}$. With this setup, the dimensions of $U$, $W$, and $V$ are $100 \\times 65$, $100 \\times 100$, and $65 \\times 100$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we must compute a softmax to obtain the RNN output. For a vector $\\mathbf{x}$, we have:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(\\mathbf{x})_i = \\frac{e^{\\mathbf{x}_i}}{\\sum_j e^{\\mathbf{x}_j}}\n",
    "$$\n",
    "\n",
    "Below, we show a simple trick to improve numerical stability of the softmax.  \n",
    "Afterwards, we define the RNN model and its initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: not numerically stable\n",
    "def softmax_unstable(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pkgs/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0., nan])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_unstable([1, 2, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerically stable version\n",
    "def softmax(x):\n",
    "    exponential = np.exp(x - np.max(x))\n",
    "    return exponential / np.sum(exponential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([1,2,1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return np.log(softmax(x) + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.38155106e+01, -1.38155106e+01,  9.99999500e-07])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax([1,2,1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(input_size, hidden_size, output_size):\n",
    "    params = {\n",
    "        'U': np.random.randn(hidden_size, input_size) * 0.01,\n",
    "        'W': np.random.randn(hidden_size, hidden_size) * 0.01,\n",
    "        'V': np.random.randn(output_size, hidden_size) * 0.01,\n",
    "        'b_h': np.zeros(hidden_size),\n",
    "        'b_o': np.zeros(output_size)\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_hidden(hidden_size):\n",
    "    return np.zeros(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(params, x, h_prev):\n",
    "    h = np.tanh(np.dot(params['U'], x) + np.dot(params['W'], h_prev) + params['b_h'])\n",
    "    y = softmax(np.dot(params['V'], h) + params['b_o'])\n",
    "    return y, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(output, target):\n",
    "    \"\"\"Negative log-likelihood loss. Useful for training a classification problem with n classes.\n",
    "    \"\"\"\n",
    "    output = np.log(output)\n",
    "    return -output[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, input_seq, target_seq, opts):\n",
    "    \"\"\"\n",
    "    Compute the loss of RNN based on data.\n",
    "    \n",
    "    :param params: dict of str: tensor, including keys U, W, v, b_h, b_o.\n",
    "    :param input_seq: list of str. Input string.\n",
    "    :param target_seq: list of str. Target string.\n",
    "    :param opts: dict of str: int. Including keys input_size, hidden_size, output_size.\n",
    "    \n",
    "    :return final_string: str. \n",
    "    \"\"\"\n",
    "    hidden = initialize_hidden(opts['hidden_size'])\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(len(input_seq)):\n",
    "        x = input_seq[i]\n",
    "        \n",
    "        hidden = np.tanh(np.dot(params['U'], x) + np.dot(params['W'], hidden) + params['b_h'])\n",
    "        output = softmax(np.dot(params['V'], hidden) + params['b_o'])\n",
    "\n",
    "        loss += criterion(output, target_seq[i])\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_grad = grad(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot(j, length):\n",
    "    vec = np.zeros(length)\n",
    "    vec[j] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(params, initial, length, opts):\n",
    "    \"\"\"\n",
    "    Sampling a string with a Recurrent neural network.\n",
    "    \n",
    "    :param params: dict of str: tensor, including keys U, W, v, b_h, b_o\n",
    "    :param initial: str. Beginning character.\n",
    "    :param length: length of the generated string.\n",
    "    :param opts: dict of str: int. Including keys input_size, hidden_size, output_size.\n",
    "    \n",
    "    :return final_string: str. \n",
    "    \"\"\"\n",
    "    hidden = initialize_hidden(opts['hidden_size'])\n",
    "    current_char = initial\n",
    "    final_string = initial\n",
    "    \n",
    "    for i in range(length):\n",
    "        x = create_one_hot(char_to_index[current_char], opts['input_size'])\n",
    "        output, hidden = model(params, x, hidden)\n",
    "        \n",
    "        p = output\n",
    "        current_index = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        current_char = index_to_char[current_index]\n",
    "        final_string += current_char\n",
    "    \n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Use non-overlapping 25-character chunks for training\n",
    "    sequence_length = 25\n",
    "    num_epochs = 10\n",
    "    print_every = 100\n",
    "    evaluate_every = 100\n",
    "    lr = 0.01\n",
    "\n",
    "    opts = {\n",
    "        'input_size': vocab_size,\n",
    "        'hidden_size': 100,\n",
    "        'output_size': vocab_size,\n",
    "    }\n",
    "\n",
    "    params = initialize_params(opts['input_size'], opts['hidden_size'], opts['output_size'])\n",
    "\n",
    "    for ep in range(num_epochs):\n",
    "        for i in range(data_length // sequence_length):\n",
    "            start = i * sequence_length\n",
    "            end = start + sequence_length + 1\n",
    "            chunk = text[start:end]\n",
    "\n",
    "            input_chars = chunk[:-1]\n",
    "            target_chars = chunk[1:]\n",
    "\n",
    "            input_seq = [char_to_index[c] for c in input_chars]\n",
    "            target_seq = [char_to_index[c] for c in target_chars]\n",
    "\n",
    "            input_seq_one_hot = [create_one_hot(j, vocab_size) for j in input_seq]\n",
    "\n",
    "            example_loss = loss(params, input_seq_one_hot, target_seq, opts)\n",
    "\n",
    "            grad_params = loss_grad(params, input_seq_one_hot, target_seq, opts)\n",
    "            for param in params:\n",
    "                gradient = np.clip(grad_params[param], -5, 5)\n",
    "                \n",
    "                '''\n",
    "                Note that we are clipping gradients to prevent exploding gradients.\n",
    "                As an experiment, try uncommenting the below line, and setting the\n",
    "                learning rate to a higher value (>0.1) to observe the phenomena.\n",
    "                '''\n",
    "                # gradient = grad_params[param]\n",
    "                \n",
    "                params[param] -= lr * gradient\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                print(\"LOSS = {}\".format(example_loss))\n",
    "                # print(grad_params)\n",
    "\n",
    "            if i % evaluate_every == 0:\n",
    "                sampled_string = sample(params, initial='a', length=100, opts=opts)\n",
    "                print(sampled_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Long Short-Term Memory Networks (LSTMs)\n",
    "\n",
    "The LSTM improves upon several shortcomings of the RNN, in particular by allowing gradients to flow through long sequences without vanishing or exploding as fast as they would in RNNs. We will implement the LSTM, and then apply it again to the Shakespearian text dataset.\n",
    "\n",
    "![Long Short-Term Memory Networks Diagram](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "(Image from the [LSTM Tutorial](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update of an LSTM is given by the following equations:\n",
    "\n",
    "$$\n",
    "i_t = \\sigma(U_i x_t + W_i h_{t-1} + b_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_t = \\sigma(U_f x_t + W_f h_{t-1} + b_f)\n",
    "$$\n",
    "\n",
    "$$\n",
    "o_t = \\sigma(U_o x_t + W_o h_{t-1} + b_o)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{C}_t = \\tanh(U_C x_t + W_C h_{t-1} + b_C)\n",
    "$$\n",
    "\n",
    "$$\n",
    "C_t = i_t * \\tilde{C}_t + f_t * C_{t-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_t = o_t * \\tanh(C_t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(input_size, hidden_size, output_size):\n",
    "    params = {\n",
    "        'U_i': np.random.randn(hidden_size, input_size) * 0.01,\n",
    "        'W_i': np.random.randn(hidden_size, hidden_size) * 0.01,\n",
    "        'b_i': np.zeros(hidden_size),\n",
    "        \n",
    "        'U_f': np.random.randn(hidden_size, input_size) * 0.01,\n",
    "        'W_f': np.random.randn(hidden_size, hidden_size) * 0.01,\n",
    "        'b_f': np.zeros(hidden_size),\n",
    "        \n",
    "        'U_o': np.random.randn(hidden_size, input_size) * 0.01,\n",
    "        'W_o': np.random.randn(hidden_size, hidden_size) * 0.01,\n",
    "        'b_o': np.zeros(hidden_size),\n",
    "        \n",
    "        'U_c': np.random.randn(hidden_size, input_size) * 0.01,\n",
    "        'W_c': np.random.randn(hidden_size, hidden_size) * 0.01,\n",
    "        'b_c': np.zeros(hidden_size),\n",
    "        \n",
    "        'V': np.random.randn(output_size, hidden_size) * 0.01,\n",
    "        'b': np.zeros(output_size)\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "def model(params, x, h_prev, C_prev):\n",
    "    i_t = sigmoid(np.dot(params['U_i'], x) + np.dot(params['W_i'], h_prev) + params['b_i'])\n",
    "    f_t = sigmoid(np.dot(params['U_f'], x) + np.dot(params['W_f'], h_prev) + params['b_f'])\n",
    "    o_t = sigmoid(np.dot(params['U_o'], x) + np.dot(params['W_o'], h_prev) + params['b_o'])\n",
    "    \n",
    "    C_t_tilde = np.tanh(np.dot(params['U_c'], x) + np.dot(params['W_c'], h_prev) + params['b_c'])\n",
    "    C_t = i_t * C_t_tilde + f_t * C_prev\n",
    "    h_t = o_t * np.tanh(C_t)\n",
    "    \n",
    "    y = softmax(np.dot(params['V'], h_t) + params['b'])\n",
    "    return y, h_t, C_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_hidden(hidden_size):\n",
    "    return np.zeros(hidden_size), np.zeros(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, input_seq, target_seq, opts):\n",
    "    \"\"\"\n",
    "    Compute the loss of RNN based on data.\n",
    "    \n",
    "    :param params: dict of str: tensor, including keys U, W, v, b_h, b_o.\n",
    "    :param input_seq: list of str. Input string.\n",
    "    :param target_seq: list of str. Target string.\n",
    "    :param opts: dict of str: int. Including keys input_size, hidden_size, output_size.\n",
    "    \n",
    "    :return final_string: str. \n",
    "    \"\"\"\n",
    "    hidden, cell = initialize_hidden(opts['hidden_size'])\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(len(input_seq)):\n",
    "        x = input_seq[i]\n",
    "        \n",
    "        i_t = sigmoid(np.dot(params['U_i'], x) + np.dot(params['W_i'], hidden) + params['b_i'])\n",
    "        f_t = sigmoid(np.dot(params['U_f'], x) + np.dot(params['W_f'], hidden) + params['b_f'])\n",
    "        o_t = sigmoid(np.dot(params['U_o'], x) + np.dot(params['W_o'], hidden) + params['b_o'])\n",
    "\n",
    "        C_t_tilde = np.tanh(np.dot(params['U_c'], x) + np.dot(params['W_c'], hidden) + params['b_c'])\n",
    "        cell = i_t * C_t_tilde + f_t * cell\n",
    "        hidden = o_t * np.tanh(cell)\n",
    "\n",
    "        output = softmax(np.dot(params['V'], hidden) + params['b'])\n",
    "        \n",
    "        loss += criterion(output, target_seq[i])\n",
    "    return loss\n",
    "\n",
    "loss_grad = grad(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(params, initial, length, opts):\n",
    "    \"\"\"\n",
    "    Sampling a string with a Recurrent neural network.\n",
    "    \n",
    "    :param params: dict of str: tensor, including keys U, W, v, b_h, b_o\n",
    "    :param initial: str. Beginning character.\n",
    "    :param length: length of the generated string.\n",
    "    :param opts: dict of str: int. Including keys input_size, hidden_size, output_size.\n",
    "    \n",
    "    :return final_string: str. \n",
    "    \"\"\"\n",
    "    hidden, cell = initialize_hidden(opts['hidden_size'])\n",
    "    current_char = initial\n",
    "    final_string = initial\n",
    "    \n",
    "    for i in range(length):\n",
    "        x = create_one_hot(char_to_index[current_char], opts['input_size'])\n",
    "        output, hidden, cell = model(params, x, hidden, cell)\n",
    "        \n",
    "        p = output\n",
    "        current_index = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        current_char = index_to_char[current_index]\n",
    "        final_string += current_char\n",
    "    \n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3: Batch Training w/ PyTorch Architecture Example \n",
    "\n",
    "In the next two sections, we will explore some methods to speed up RNN training. First, we will consider batch training on sequential data, in the case that data series are all of the same length.\n",
    "The RNN cell is also reimplemented in PyTorch. \n",
    "\n",
    "Note:  \n",
    "The below sampling technique should not be done in reality. This demo serves only to show how batch training is possible. Poor sampling probably contributes to why performance is slightly worse than online training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    ''' Identical to the RNN cell defined previously'''\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        hidden = self.tanh(hidden)\n",
    "        output = self.h2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(rnn, initial, length):\n",
    "    \"\"\"\n",
    "    Sampling a string with a Recurrent neural network.\n",
    "    \n",
    "    :param rnn: PyTorch RNN model\n",
    "    :param initial: str. Beginning character.\n",
    "    :param length: length of the generated string.\n",
    "    \n",
    "    :return final_string: str. \n",
    "    \"\"\"\n",
    "    \n",
    "    current_char = initial\n",
    "    final_string = initial\n",
    "    \n",
    "    hidden = rnn.initHidden(1).to(device)\n",
    "    \n",
    "    for i in range(length):\n",
    "        x = create_one_hot(char_to_index[current_char], 65)\n",
    "        \n",
    "        # Create 1 char tensor\n",
    "        x = torch.tensor(x).unsqueeze(0).float().to(device)\n",
    "        output, hidden = rnn.forward(x, hidden)\n",
    "        \n",
    "        p = np.exp(output.cpu().detach().numpy())\n",
    "        current_index = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        current_char = index_to_char[current_index]\n",
    "        final_string += current_char\n",
    "    \n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(text, seq_len, batch_size, vocab_size, idx):\n",
    "    ''' \n",
    "    Sequentially defines non-overlapping batches defined by index.\n",
    "    Ignores last bit of text. Don't do this in practice.\n",
    "    A better sampling strategy would shuffle data. \n",
    "    This is not implemented for brevity.\n",
    "    '''\n",
    "    start = idx * seq_len * batch_size\n",
    "    end = (idx + 1) * seq_len * batch_size + 1\n",
    "    \n",
    "    chunk = text[start:end]\n",
    "    input_chars = chunk[:-1]\n",
    "    target_chars = chunk[1:]\n",
    "    \n",
    "    input_seq = [char_to_index[c] for c in input_chars]\n",
    "    target_seq = [char_to_index[c] for c in target_chars]\n",
    "\n",
    "    input_seq_one_hot = [create_one_hot(j, vocab_size) for j in input_seq]\n",
    "    \n",
    "    # Convert chunks of text to batches of sequential text\n",
    "    input_tensor = torch.tensor(input_seq_one_hot).reshape(-1, batch_size, vocab_size)\n",
    "    target_tensor = torch.tensor(target_seq).reshape(-1, batch_size)\n",
    "    \n",
    "    # Send to GPU\n",
    "    input_tensor = input_tensor.float().to(device)\n",
    "    target_tensor = target_tensor.to(device)\n",
    "    \n",
    "    return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_hidden = 100\n",
    "seq_len = 25\n",
    "batch_size = 8\n",
    "\n",
    "n_epochs = 5\n",
    "lr = 0.001\n",
    "\n",
    "rnn = RNN(vocab_size, n_hidden, vocab_size).to(device)\n",
    "\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=lr)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "for _ in range(n_epochs):\n",
    "    for i in range(data_length // (batch_size * seq_len)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get batches\n",
    "        batch_input, batch_target = get_batch(text, seq_len, batch_size, vocab_size, i)\n",
    "\n",
    "        # Initialize hidden state\n",
    "        hidden = rnn.initHidden(batch_size).to(device)\n",
    "\n",
    "        # Stores losses\n",
    "        total_loss = 0\n",
    "        for j in range(seq_len):\n",
    "            # Get RNN output for batch\n",
    "            output, hidden = rnn.forward(batch_input[j], hidden)\n",
    "            # Calculate loss for batch\n",
    "            loss = criterion(output, batch_target[j])\n",
    "            total_loss += loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i % 500 == 0):\n",
    "            print(\"Training loss: {}\\n\".format(total_loss))\n",
    "            s = sample(rnn, 'a', 100)\n",
    "            print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Batch Training with Irregular Sequences\n",
    "\n",
    "One strength of the RNN is the ability to handle irregularly length sequences. However, this poses a challenge at train time. How do we iterate over the RNN with multiple sequences of different length? Two strategies exist:\n",
    "\n",
    "- Online Training: Simply train one sequence at a time. Easy to implement, but slow\n",
    "- Batch Training with Padding: Pad batch of sequences with zeros to match length of longest sequence in batch.\n",
    "\n",
    "Online Training was shown in Part 1 / 2.  \n",
    "Batch Training with Padding will be shown below using PyTorch's `pack_padded_sequence`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define generic RNN model\n",
    "rnn = nn.RNN(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Input is: [[[1], [3], [3], [4]], [[4], [1]], [[7]]] \n",
      "\n",
      "Padded Input is: [[[1], [3], [3], [4]], [[4], [1], [0], [0]], [[7], [0], [0], [0]]] \n",
      "\n",
      "PackedSequence(data=tensor([[1.],\n",
      "        [4.],\n",
      "        [7.],\n",
      "        [3.],\n",
      "        [1.],\n",
      "        [3.],\n",
      "        [4.]]), batch_sizes=tensor([3, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "tensor([[[0.0819],\n",
      "         [0.9541],\n",
      "         [0.9987]],\n",
      "\n",
      "        [[0.8601],\n",
      "         [0.2624],\n",
      "         [0.0000]],\n",
      "\n",
      "        [[0.8949],\n",
      "         [0.0000],\n",
      "         [0.0000]],\n",
      "\n",
      "        [[0.9674],\n",
      "         [0.0000],\n",
      "         [0.0000]]], grad_fn=<CopySlices>)\n",
      "tensor([4, 2, 1])\n",
      "Final output is: [tensor([0.9674], grad_fn=<SelectBackward>), tensor([0.2624], grad_fn=<SelectBackward>), tensor([0.9987], grad_fn=<SelectBackward>)]\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "'''\n",
    "The point of this exercise is to show padding, so we'll use a dummy batch of data.\n",
    "Note irregular length of sequences. We use batches of dimension B x L x I where:\n",
    "B = batch size, L = sequence length, I = input features\n",
    "For this example, a batch has 3 samples with a max sequence length of 4, and 1 input feature.\n",
    "'''\n",
    "dummy_batch = [\n",
    "    [[1], [3], [3], [4]],\n",
    "    [[4], [1]],\n",
    "    [[7]]\n",
    "]\n",
    "\n",
    "print(\"Original Input is:\", dummy_batch, \"\\n\")\n",
    "\n",
    "'''\n",
    "Notice that we can't easily input this to an RNN, or even represent it as a tensor.\n",
    "We'll use padding to circumvent this.\n",
    "'''\n",
    "\n",
    "# Store original lengths of sequences before padding\n",
    "orig_len = [len(x) for x in dummy_batch]\n",
    "\n",
    "# Pad with zeros\n",
    "max_len = len(max(dummy_batch, key=len))\n",
    "\n",
    "for i in range(len(dummy_batch)):\n",
    "    zero_pad = [[0]] * (max_len - len(dummy_batch[i]))\n",
    "    dummy_batch[i] += zero_pad\n",
    "    \n",
    "print(\"Padded Input is:\", dummy_batch, \"\\n\")\n",
    "\n",
    "# Convert to tensor and pack\n",
    "dummy_tensor = torch.tensor(dummy_batch).float()\n",
    "packed_tensor = pack_padded_sequence(dummy_tensor, orig_len, batch_first=True)\n",
    "\n",
    "print(packed_tensor)\n",
    "\n",
    "output, _ = rnn(packed_tensor)\n",
    "\n",
    "# Apply the inverse transformation to recover the outputs\n",
    "output = pad_packed_sequence(output)\n",
    "\n",
    "# Notice that the output contains zeros where we added zero padding!\n",
    "print(output[0])\n",
    "# Reconstructed output also contains original indices\n",
    "print(output[1])\n",
    "\n",
    "# To recover the true output from where the input exists, just use the output\n",
    "# from the range of the original input. We show an example for one sequence\n",
    "true_output = []\n",
    "for i in range(len(dummy_batch)):\n",
    "    orig_len = output[1][i]\n",
    "    true_output.append(output[0][orig_len-1, i])\n",
    "\n",
    "print(\"Final output is:\", true_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
