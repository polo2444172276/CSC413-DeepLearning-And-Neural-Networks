{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "a4-stylegan_solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YkGmGCi6oRu"
      },
      "source": [
        "## **CSC413 - Programming Assignment 4 - StyleGAN2-Ada Experiment**\n",
        "\n",
        "Disclaimer: Codes are borrowed from StyleGAN official documentation on Github https://github.com/NVlabs/stylegan\n",
        "\n",
        "Make sure to set your runtime to GPU\n",
        "\n",
        "Remember to save your progress periodically!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-W5eReUVPnS"
      },
      "source": [
        "# Run this for Google CoLab (use TensorFlow 1.x)\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "# clone stylegan 2\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVNM_ERtVxA1"
      },
      "source": [
        "#setup some environments (Do not change any of the following)\n",
        "import sys\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "from IPython.display import Image\n",
        "import PIL.Image\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sys.path.insert(0, \"/content/stylegan2-ada\") #do not remove this line\n",
        "\n",
        "import dnnlib\n",
        "import dnnlib.tflib as tflib\n",
        "import IPython.display\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhU-cjsC6CdN"
      },
      "source": [
        "Next, we will load a pre-trained StyleGan2-ada network.\n",
        "\n",
        "Each of the following pre-trained network is specialized to generate one type of image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENrDl7ZTddyT"
      },
      "source": [
        "# Uncomment one of the following URL to begin\n",
        "\n",
        "#URL = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl\"      # Human faces\n",
        "#URL = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/cifar10.pkl\"  # CIFAR10, these images are a bit too tiny for our experiment \n",
        "URL = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/afhqwild.pkl\" # wild animal pictures\n",
        "#URL = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/metfaces.pkl\" # European portrait paintings\n",
        "#URL = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/afhqcat.pkl\"  # cats\n",
        "#URL = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/afhqdog.pkl\"  # dogs\n",
        "tflib.init_tf() #this creates a default Tensorflow session\n",
        "\n",
        "# we are now going to load the StyleGAN2-Ada model \n",
        "with dnnlib.util.open_url(URL) as fp:\n",
        "    _G, _D, Gs = pickle.load(fp) \n",
        "# Here is a brief description of _G, _D, Gs, for details see the official StyleGAN documentation \n",
        "# _G = Instantaneous snapshot of the generator. Mainly useful for resuming a previous training run.\n",
        "# _D = Instantaneous snapshot of the discriminator. Mainly useful for resuming a previous training run.\n",
        "# Gs = Long-term average of the generator. Yields higher-quality results than the instantaneous snapshot.\n",
        "# We will work with Gs "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk3EKnIyckPE"
      },
      "source": [
        "## Part 1 Sampling and Identifying Fakes \n",
        "\n",
        "Open: https://github.com/NVlabs/stylegan and follow the instructions starting from *There are three ways to use the pre-trained generator....*\n",
        "\n",
        "Complete generate_latent_code and generate_images function in the Colab notebook to generate a small row of $3 - 5$ images. \n",
        "\n",
        "You do not need to include these images into your PDF submission. \n",
        "\n",
        "If you wish, you can try to use https://www.whichfaceisreal.com/learn.html as a guideline to spot any imperfections that you detect in these images, e.g., ``blob artifact\" and make a short remark for your attached images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7eAfVJDiB0y"
      },
      "source": [
        "# Sample a batch of latent codes {z_1, ...., z_B}, B is your batch size.  \n",
        "def generate_latent_code(SEED, BATCH, LATENT_DIMENSION = 512):\n",
        "  \"\"\"\n",
        "  This function returns a sample a batch of 512 dimensional random latent code\n",
        "\n",
        "  - SEED: int\n",
        "  - BATCH: int that specifies the number of latent codes, Recommended batch_size is 3 - 6\n",
        "  - LATENT_DIMENSION is by default 512 (see Karras et al.)\n",
        "  \n",
        "  You should use np.random.RandomState to construct a random number generator, say rnd\n",
        "  Then use rnd.randn along with your BATCH and LATENT_DIMENSION to generate your latent codes. \n",
        "  This samples a batch of latent codes from a normal distribution \n",
        "  https://numpy.org/doc/stable/reference/random/generated/numpy.random.RandomState.randn.html\n",
        "  \n",
        "  Return latent_codes, which is a 2D array with dimensions BATCH times LATENT_DIMENSION \n",
        "  \"\"\"\n",
        "  ################################################################################\n",
        "  ########################## COMPLETE THE FOLLOWING ##############################\n",
        "  ################################################################################\n",
        "  rnd = np.random.RandomState(SEED)\n",
        "  latent_codes = rnd.randn(BATCH, LATENT_DIMENSION)\n",
        "  ################################################################################\n",
        "  return latent_codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8giN3BfibqG"
      },
      "source": [
        "# Sample images from your latent codes https://github.com/NVlabs/stylegan\n",
        "# You can use their default settings\n",
        "\n",
        "################################################################################\n",
        "########################## COMPLETE THE FOLLOWING ##############################\n",
        "################################################################################\n",
        "def generate_images(SEED, BATCH, TRUNCATION = 0.7):\n",
        "  \"\"\"\n",
        "  This function generates a batch of images from latent codes. \n",
        "  \n",
        "  - SEED: int\n",
        "  - BATCH: int that specifies the number of latent codes to be generated\n",
        "  - TRUNCATION: float between [-1, 1] that decides the amount of clipping to apply to the latent code distribution\n",
        "              recommended setting is 0.7\n",
        "\n",
        "  You will use Gs.run() to sample images. See https://github.com/NVlabs/stylegan for details\n",
        "  You may use their default setting.  \n",
        "  \"\"\" \n",
        "  # Sample a batch of latent code z using generate_latent_code function\n",
        "  latent_codes = generate_latent_code(SEED, BATCH)\n",
        "\n",
        "  # Convert latent code into images by following https://github.com/NVlabs/stylegan\n",
        "  fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True) \n",
        "  images = Gs.run(latent_codes,\n",
        "                  None,   \n",
        "                  truncation_psi=TRUNCATION, \n",
        "                  randomize_noise=False,\n",
        "                  output_transform=fmt\n",
        "                  )\n",
        "  return PIL.Image.fromarray(np.concatenate(images, axis=1) , 'RGB')\n",
        "################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdaxzuAFcTM8"
      },
      "source": [
        "# Generate your images\n",
        "generate_images(677, 3, 0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9Jre5OXbUHD"
      },
      "source": [
        "## **Part 2 Interpolation**\n",
        "\n",
        "Complete the interpolate_images function using linear interpolation between two latent codes,\n",
        "\\begin{equation}\n",
        "    z = r z_1 + (1-r) z_2,  r \\in [0, 1]\n",
        "\\end{equation}\n",
        "and feeding this interpolation through the StyleGAN2-Ada generator Gs as done in generate_images. Include a small row of interpolation in your PDF submission as a screen shot if necessary to keep the file size small. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUwCVd9mlDOr"
      },
      "source": [
        "################################################################################\n",
        "########################## COMPLETE THE FOLLOWING ##############################\n",
        "################################################################################\n",
        "def interpolate_images(SEED1, SEED2, INTERPOLATION, BATCH = 1, TRUNCATION = 0.7):\n",
        "  \"\"\"\n",
        "  - SEED1, SEED2: int, seed to use to generate the two latent codes\n",
        "  - INTERPOLATION: int, the number of interpolation between the two images, recommended setting 6 - 10\n",
        "  - BATCH: int, the number of latent code to generate. In this experiment, it is 1. \n",
        "  - TRUNCATION: float between [-1, 1] that decides the amount of clipping to apply to the latent code distribution\n",
        "              recommended setting is 0.7\n",
        "\n",
        "  You will interpolate between two latent code that you generate using the above formula\n",
        "  You can generate an interpolation variable using np.linspace\n",
        "  https://numpy.org/doc/stable/reference/generated/numpy.linspace.html\n",
        "  \n",
        "  This function should return an interpolated image. Include a screenshot in your submission.\n",
        "  \"\"\"\n",
        "  latent_code_1 = generate_latent_code(SEED1, BATCH)\n",
        "  latent_code_2 = generate_latent_code(SEED2, BATCH)\n",
        "  percent_first_noise = np.linspace(0, 1, INTERPOLATION)[:, None]\n",
        "  interpolated_noise = latent_code_1 * percent_first_noise + latent_code_2 * (1 - percent_first_noise)\n",
        "  print(np.shape(interpolated_noise))\n",
        "  fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True) # Specify the desired output format and shape\n",
        "  images = Gs.run(interpolated_noise,\n",
        "                  None,    \n",
        "                  truncation_psi=TRUNCATION, \n",
        "                  randomize_noise=False,\n",
        "                  output_transform=fmt\n",
        "                  )\n",
        "  \n",
        "  return PIL.Image.fromarray(np.concatenate(images, axis=1) , 'RGB')\n",
        "################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV4Df5J8lZLy"
      },
      "source": [
        "# Create an interpolation of your generated images\n",
        "interpolate_images(1092, 123, 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV2tsaAhaTzg"
      },
      "source": [
        "## **Part 3 Style Mixing and Fine Control**\n",
        "In the final part, you will reproduce the famous style mixing example from the original StyleGAN paper.\n",
        "\n",
        "We will first learn how to generate from sub-networks of the StyleGAN generator. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG-xbj9qtMTA"
      },
      "source": [
        "# The above code generates images but from sub-networks of the StyleGAN generator\n",
        "# Complete the function by following \\url{https://github.com/NVlabs/stylegan} \n",
        "# And Look up Gs.components.mapping,  Gs.components.synthesism, Gs.get_var\n",
        "# Remember to use the truncation trick as described in the handout\n",
        "def generate_from_subnetwork(src_seeds, LATENT_DIMENSION = 512):\n",
        "    \"\"\"\n",
        "    - src_seeds: a list of int, where each int is used to generate a latent code, e.g., [1,2,3]\n",
        "    - LATENT_DIMENSION: by default 512\n",
        "\n",
        "    You will complete the code snippet in the Write Your Code Here block\n",
        "    This generates several images from a sub-network of the genrator. \n",
        "\n",
        "    To prevent mistakes, we have provided the variable names which corresponds to the ones in the StyleGAN documentation\n",
        "    You should use their convention. \n",
        "    \"\"\"\n",
        "\n",
        "    # default arguments to Gs.components.synthesis.run, this is given to you. \n",
        "    synthesis_kwargs = {\n",
        "        'output_transform': dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True),\n",
        "        'randomize_noise': False,\n",
        "        'minibatch_size': 4\n",
        "    }\n",
        "    ############################################################################\n",
        "    ########################## WRITE YOUR CODE HERE ############################\n",
        "    ############################################################################\n",
        "    truncation = 0.7\n",
        "    src_latents = np.stack([np.random.RandomState(seed).randn(LATENT_DIMENSION) for seed in src_seeds])  \n",
        "    src_dlatents = Gs.components.mapping.run(src_latents, None) # [minibatch, layer, component]\n",
        "    w_avg = Gs.get_var('dlatent_avg') #get average latent vector\n",
        "    src_dlatents = w_avg + (src_dlatents - w_avg) * truncation # truncation trick on your latent \n",
        "    all_images = Gs.components.synthesis.run(src_dlatents, **synthesis_kwargs)\n",
        "    ############################################################################\n",
        "    return PIL.Image.fromarray(np.concatenate(all_images, axis=1) , 'RGB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BI0vEoYdtbJk"
      },
      "source": [
        "# generate several iamges from the sub-network\n",
        "generate_from_subnetwork([1,2,3,4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QOHpSmoGrZI"
      },
      "source": [
        "Initialize the col_seeds, row_seeds and col_styles and generate a grid of image. A recommended example for your experiment is as follows:\n",
        "\n",
        "*  col_seeds = [1, 2, 3, 4, 5]\n",
        "*  row_seeds = [6]\n",
        "*  col_styles = [1, 2, 3, 4, 5]\n",
        "\n",
        "and\n",
        "\n",
        "*  col_seeds = [1, 2, 3, 4, 5]\n",
        "*  row_seeds = [6]\n",
        "*  col_styles = [8, 9, 10, 11, 12]\n",
        "\n",
        "You will then incorporate your code from generate from sub_network into the cell below. \n",
        "\n",
        "Experiment with the col_styles variable. Explain what col_styles does, for instance, roughly describe what these numbers corresponds to. Create a simple experiment to backup your argument. Include **at maximum two** sets of images that illustrates the effect of changing col_styles and your explanation. Include them as screen shots to minimize the size of the file.\n",
        "\n",
        "Make reference to the original StyleGAN or the StyleGAN2 paper by Karras et al. as needed https://arxiv.org/pdf/1812.04948.pdf https://arxiv.org/pdf/1912.04958.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM4QvAkuwCjW"
      },
      "source": [
        "####################COMPLETE THE NEXT THREE LINES###############################\n",
        "col_seeds = [1, 2, 3, 4, 5]\n",
        "row_seeds = [6]\n",
        "col_styles = [1, 2, 3, 4]\n",
        "################################################################################\n",
        "src_seeds = list(set(row_seeds + col_seeds))\n",
        "\n",
        "# default arguments to Gs.components.synthesis.run, do not change\n",
        "synthesis_kwargs = {\n",
        "    'output_transform': dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True),\n",
        "    'randomize_noise': False,\n",
        "    'minibatch_size': 4\n",
        "}\n",
        "\n",
        "########################## COMPLETE THE FOLLOWING ##############################\n",
        "# Copy the   #### WRITE YOUR CODE HERE #### portion from generate_from_subnetwork()\n",
        "truncation = 0.7\n",
        "src_latents = np.stack([np.random.RandomState(seed).randn(512) for seed in src_seeds])  \n",
        "src_dlatents = Gs.components.mapping.run(src_latents, None) \n",
        "w_avg = Gs.get_var('dlatent_avg') # get the average latent vector \n",
        "src_dlatents = w_avg + (src_dlatents - w_avg) * truncation # truncation trick on your latent \n",
        "all_images = Gs.components.synthesis.run(src_dlatents, **synthesis_kwargs) \n",
        "################################################################################\n",
        "\n",
        "# (Do not change)\n",
        "image_dict = {(seed, seed): image for seed, image in zip(src_seeds, list(all_images))}\n",
        "w_dict = {seed: w for seed, w in zip(src_seeds, list(src_dlatents))} \n",
        "\n",
        "# Generating Images (Do not Change)\n",
        "for row_seed in row_seeds:\n",
        "    for col_seed in col_seeds:\n",
        "        w = w_dict[row_seed].copy()\n",
        "        w[col_styles] = w_dict[col_seed][col_styles]\n",
        "        image = Gs.components.synthesis.run(w[np.newaxis], **synthesis_kwargs)[0]\n",
        "        image_dict[(row_seed, col_seed)] = image\n",
        "\n",
        "# Create an Image Grid (Do not Change)\n",
        "def create_grid_images(): \n",
        "  _N, _C, H, W = Gs.output_shape\n",
        "  canvas = PIL.Image.new('RGB', (W * (len(col_seeds) + 1), H * (len(row_seeds) + 1)), 'black')\n",
        "  for row_idx, row_seed in enumerate([None] + row_seeds):\n",
        "      for col_idx, col_seed in enumerate([None] + col_seeds):\n",
        "          if row_seed is None and col_seed is None:\n",
        "              continue\n",
        "          key = (row_seed, col_seed)\n",
        "          if row_seed is None:\n",
        "              key = (col_seed, col_seed)\n",
        "          if col_seed is None:\n",
        "              key = (row_seed, row_seed)\n",
        "          canvas.paste(PIL.Image.fromarray(image_dict[key], 'RGB'), (W * col_idx, H * row_idx))\n",
        "  return canvas\n",
        "\n",
        "# The following code will create your image, save it as a png, and display the image\n",
        "# Run the following code after you have set your row_seed, col_seed and col_style\n",
        "image_grid = create_grid_images()\n",
        "image_grid.save('image_grid.png')\n",
        "im = Image.open(\"image_grid.png\")\n",
        "im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgopBE4ROMR8"
      },
      "source": [
        "The following is a generic summary should you lost points on this question: This answer is missing one or more of the following explanations. According to the StyleGAN paper as well as tutorial videos, the coarse, medium, and fine details from the Column images, as specified by the 'col_styles' (which refers to Column Styles), are combined with the rest of the features from the Row image. There is no correlation between col_styles and col_seeds or row_seeds. These variables can be independently chosen.\n",
        "\n",
        "Coarse details of the Colum images are associated with the integers 0, 1, 2, 3. Middle/medium details of the Column images are associated with integers 4, 5, 6, 7. And fine details from the Column images are associated with values 8 - max_styles, where max_styles will depend on the generated image size of your model. Although we do not demand this level of detail, it should be obvious from your experiment that the given values correspond to at least coarse or fine details of the generated image, being transferred from the Column images to the Row images.\n",
        "\n",
        "Also, your answer/observation might not be generalizable across datasets. If you switch out Human Face dataset for Dogs and Cats, will your answer still hold? For example, it does not make (the most) sense to refer to the 'hair style' or 'race' of cats and dogs, or 'fur color' of humans."
      ]
    }
  ]
}